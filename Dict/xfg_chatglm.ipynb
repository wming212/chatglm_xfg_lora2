{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNqffLsNXc05nLIVYZEi7oC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wming212/chatglm_xfg_lora2/blob/main/Dict/xfg_chatglm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, tarfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def make_targz_one_by_one(output_filename, source_dir):\n",
        "  tar = tarfile.open(output_filename,\"w\")\n",
        "  for root,dir_name,files_list in os.walk(source_dir):\n",
        "    for file in files_list:\n",
        "      pathfile = os.path.join(root, file)\n",
        "      tar.add(pathfile)\n",
        "  tar.close()\n",
        "\n",
        "  files.download(output_filename)\n",
        "\n",
        "make_targz_one_by_one('chatglm2-6b-lora', '/content/chatglm2-6b')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ukxT-f0TBf7A",
        "outputId": "0b52c5e4-da8a-488b-ed79-1440e466403e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bfb1a51e-0f7a-4ef0-ab50-2cff11f3cfef\", \"chatglm2-6b-lora\", 24978821120)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, tarfile\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def make_targz_one_by_one(output_filename, source_dir):\n",
        "  tar = tarfile.open(output_filename,\"w\")\n",
        "  for root,dir_name,files_list in os.walk(source_dir):\n",
        "    for file in files_list:\n",
        "      pathfile = os.path.join(root, file)\n",
        "      tar.add(pathfile)\n",
        "  tar.close()\n",
        "\n",
        "  files.download(output_filename)\n",
        "\n",
        "make_targz_one_by_one('xfg_paper', '/content/xfg-paper')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "hDHpx5CsA6Pw",
        "outputId": "169d7cf0-574b-4633-e890-ec10c349c1cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_511713c4-0c68-440e-8286-1b725e324b94\", \"xfg_paper\", 59627520)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/xfg-paper\n",
        "!bash xfg_train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZeFnxR3YkHv",
        "outputId": "cd0a8b0c-90fe-4f67-9408-2de0c6f162b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/xfg-paper\n",
            "2023-12-17 07:48:48.619920: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-17 07:48:48.620056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-17 07:48:48.635234: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-17 07:48:49.740653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "12/17/2023 07:48:52 - WARNING - pet.core.parse - We recommend enable fp16 mixed precision training for ChatGLM-6B.\n",
            "12/17/2023 07:48:52 - WARNING - pet.core.parse - `ddp_find_unused_parameters` needs to be set as False in DDP training.\n",
            "12/17/2023 07:48:52 - INFO - pet.core.parse - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, 16-bits training: False\n",
            "12/17/2023 07:48:52 - INFO - pet.core.parse - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./output/label_xfg/runs/Dec17_07-48-52_6dde3750ac02,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=./output/label_xfg,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./output/label_xfg,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/17/2023 07:48:52 - INFO - dsets.loader - Loading dataset paper_label.json...\n",
            "12/17/2023 07:48:52 - WARNING - dsets.loader - Checksum failed: missing SHA-1 hash value in dataset_info.json or too many files.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2088: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
            "  warnings.warn(\n",
            "Using custom data configuration default-5bd55c45dfc99406\n",
            "12/17/2023 07:48:52 - INFO - datasets.builder - Using custom data configuration default-5bd55c45dfc99406\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "12/17/2023 07:48:52 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "12/17/2023 07:48:52 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "12/17/2023 07:48:52 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 12945.38it/s]\n",
            "Downloading took 0.0 min\n",
            "12/17/2023 07:48:52 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "12/17/2023 07:48:52 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 45.93it/s]\n",
            "Generating train split\n",
            "12/17/2023 07:48:52 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 6000 examples [00:00, 25390.17 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "12/17/2023 07:48:53 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "12/17/2023 07:48:53 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "[INFO|tokenization_utils_base.py:1821] 2023-12-17 07:48:53,093 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:1821] 2023-12-17 07:48:53,093 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:1821] 2023-12-17 07:48:53,093 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1821] 2023-12-17 07:48:53,093 >> loading file tokenizer_config.json\n",
            "[INFO|configuration_utils.py:667] 2023-12-17 07:48:53,131 >> loading configuration file /content/chatglm2-6b/config.json\n",
            "[INFO|configuration_utils.py:667] 2023-12-17 07:48:53,134 >> loading configuration file /content/chatglm2-6b/config.json\n",
            "[INFO|configuration_utils.py:725] 2023-12-17 07:48:53,135 >> Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"/content/chatglm2-6b\",\n",
            "  \"add_bias_linear\": false,\n",
            "  \"add_qkv_bias\": true,\n",
            "  \"apply_query_key_layer_scaling\": true,\n",
            "  \"apply_residual_connection_post_layernorm\": false,\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"attention_softmax_in_fp32\": true,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForCausalLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSequenceClassification\": \"modeling_chatglm.ChatGLMForSequenceClassification\"\n",
            "  },\n",
            "  \"bias_dropout_fusion\": true,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"ffn_hidden_size\": 13696,\n",
            "  \"fp32_residual_connection\": false,\n",
            "  \"hidden_dropout\": 0.0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"kv_channels\": 128,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"multi_query_attention\": true,\n",
            "  \"multi_query_group_num\": 2,\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"original_rope\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"padded_vocab_size\": 65024,\n",
            "  \"post_layer_norm\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 0,\n",
            "  \"rmsnorm\": true,\n",
            "  \"seq_length\": 32768,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.30.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 65024\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2575] 2023-12-17 07:48:53,227 >> loading weights file /content/chatglm2-6b/pytorch_model.bin.index.json\n",
            "[INFO|configuration_utils.py:577] 2023-12-17 07:48:53,228 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.30.1\"\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 7/7 [01:01<00:00,  8.85s/it]\n",
            "[INFO|modeling_utils.py:3295] 2023-12-17 07:49:55,260 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:3303] 2023-12-17 07:49:55,260 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /content/chatglm2-6b.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2927] 2023-12-17 07:49:55,263 >> Generation config file not found, using a generation config created from the model config.\n",
            "12/17/2023 07:49:55 - INFO - pet.core.adapter - Fine-tuning method: LoRA\n",
            "trainable params: 1949696 || all params: 6245533696 || trainable%: 0.0312\n",
            "Running tokenizer on dataset:   0% 0/6000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7d1a169a12fa52e1.arrow\n",
            "12/17/2023 07:49:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-5bd55c45dfc99406/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-7d1a169a12fa52e1.arrow\n",
            "Running tokenizer on dataset: 100% 6000/6000 [00:13<00:00, 432.42 examples/s]\n",
            "input_ids:\n",
            "[64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 17275, 6652, 2128, 354, 323, 260, 3075, 2231, 2854, 2580, 289, 267, 1845, 2854, 3211, 293, 10830, 30932, 4836, 30910, 30939, 400, 30910, 30940, 30932, 267, 1762, 323, 267, 2854, 3211, 293, 10830, 11488, 6955, 30954, 15858, 860, 13931, 2284, 4762, 332, 24152, 293, 4737, 1024, 7346, 14530, 3929, 30954, 14582, 283, 260, 7163, 326, 374, 285, 25433, 356, 304, 536, 586, 13535, 823, 30932, 393, 6820, 30954, 30952, 6236, 267, 792, 290, 9271, 586, 11617, 293, 7224, 14442, 30932, 8653, 293, 21338, 25184, 705, 1112, 2307, 5024, 289, 1524, 293, 1617, 5558, 1034, 4762, 14866, 388, 1034, 19897, 30930, 2284, 16726, 293, 593, 1034, 3727, 383, 7636, 10944, 267, 792, 290, 11309, 14442, 289, 794, 537, 11173, 548, 8390, 30930, 445, 434, 636, 30932, 379, 6781, 597, 2963, 289, 284, 11309, 1863, 326, 374, 285, 4977, 20287, 343, 4077, 7224, 293, 9271, 586, 930, 20735, 289, 2602, 267, 22019, 1675, 289, 1097, 293, 1524, 290, 5558, 1034, 4762, 30930, 353, 20287, 323, 11011, 290, 260, 3890, 30941, 19798, 1802, 30910, 30943, 30930, 30970, 30952, 14150, 7656, 2092, 343, 457, 330, 14777, 16219, 422, 3890, 30930, 15509, 457, 1675, 30782, 20618, 16506, 293, 7224, 422, 7937, 3890, 30065, 331, 267, 4103, 1096, 7626, 289, 26425, 4467, 1753, 1738, 30930, 657, 1508, 267, 1212, 4230, 9949, 428, 260, 1022, 911, 2060, 7177, 356, 267, 833, 290, 4070, 8653, 293, 21338, 25184, 6612, 30932, 1034, 6917, 293, 10956, 2723, 30932, 293, 5223, 30930, 657, 6833, 267, 1022, 911, 2060, 422, 16919, 769, 8390, 1034, 19897, 30930, 1043, 30932, 25833, 6612, 14383, 293, 4173, 1863, 326, 374, 285, 293, 9271, 586, 12154, 8390, 18034, 30930, 11207, 428, 260, 451, 1619, 5785, 8552, 343, 597, 1863, 326, 374, 285, 2963, 323, 2718, 30932, 2293, 289, 792, 30932, 293, 18058, 6470, 293, 11175, 650, 10944, 5558, 1034, 4762, 30930, 13, 13, 55437, 31211, 30910, 30940, 2]\n",
            "inputs:\n",
            "[Round 1]\n",
            "\n",
            "问：Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title and abstract -->title:Accessible Visual Artworks for Blind and Visually Impaired People: Comparing a Multimodal Approach with Tactile Graphics,abstract:Despite the use of tactile graphics and audio guides, blind and visually impaired people still face challenges to experience and understand visual artworks independently at art exhibitions. Art museums and other art places are increasingly exploring the use of interactive guides to make their collections more accessible. In this work, we describe our approach to an interactive multimodal guide prototype that uses audio and tactile modalities to improve the autonomous access to information and experience of visual artworks. The prototype is composed of a touch-sensitive 2.5D artwork relief model that can be freely explored by touch. Users can access localized verbal descriptions and audio by performing touch gestures on the surface while listening to themed background music along. We present the design requirements derived from a formative study realized with the help of eight blind and visually impaired participants, art museum and gallery staff, and artists. We extended the formative study by organizing two accessible art exhibitions. There, eighteen participants evaluated and compared multimodal and tactile graphic accessible exhibits. Results from a usability survey indicate that our multimodal approach is simple, easy to use, and improves confidence and independence when exploring visual artworks.\n",
            "\n",
            "答： 0\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 30940, 2]\n",
            "labels:\n",
            "0\n",
            "[INFO|trainer.py:1786] 2023-12-17 07:50:13,689 >> ***** Running training *****\n",
            "[INFO|trainer.py:1787] 2023-12-17 07:50:13,689 >>   Num examples = 6,000\n",
            "[INFO|trainer.py:1788] 2023-12-17 07:50:13,689 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1789] 2023-12-17 07:50:13,689 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1790] 2023-12-17 07:50:13,689 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1791] 2023-12-17 07:50:13,689 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1792] 2023-12-17 07:50:13,689 >>   Total optimization steps = 1,125\n",
            "[INFO|trainer.py:1793] 2023-12-17 07:50:13,690 >>   Number of trainable parameters = 1,949,696\n",
            "{'loss': 219.8295, 'learning_rate': 4.999025287600886e-05, 'epoch': 0.03}\n",
            "{'loss': 0.0, 'learning_rate': 4.996101910454953e-05, 'epoch': 0.05}\n",
            "{'loss': 0.0, 'learning_rate': 4.991232148123761e-05, 'epoch': 0.08}\n",
            "{'loss': 0.0, 'learning_rate': 4.984419797901491e-05, 'epoch': 0.11}\n",
            "{'loss': 0.0, 'learning_rate': 4.975670171853926e-05, 'epoch': 0.13}\n",
            "{'loss': 0.0, 'learning_rate': 4.964990092676263e-05, 'epoch': 0.16}\n",
            "{'loss': 0.0, 'learning_rate': 4.952387888372979e-05, 'epoch': 0.19}\n",
            "{'loss': 0.0, 'learning_rate': 4.937873385763908e-05, 'epoch': 0.21}\n",
            "{'loss': 0.0, 'learning_rate': 4.9214579028215776e-05, 'epoch': 0.24}\n",
            "{'loss': 0.0, 'learning_rate': 4.9031542398457974e-05, 'epoch': 0.27}\n",
            "{'loss': 0.0, 'learning_rate': 4.882976669482367e-05, 'epoch': 0.29}\n",
            "{'loss': 0.0, 'learning_rate': 4.860940925593703e-05, 'epoch': 0.32}\n",
            "{'loss': 0.0, 'learning_rate': 4.837064190990036e-05, 'epoch': 0.35}\n",
            "{'loss': 0.0, 'learning_rate': 4.8113650840307834e-05, 'epoch': 0.37}\n",
            "{'loss': 0.0, 'learning_rate': 4.783863644106502e-05, 'epoch': 0.4}\n",
            "{'loss': 0.0, 'learning_rate': 4.754581316012785e-05, 'epoch': 0.43}\n",
            "{'loss': 0.0, 'learning_rate': 4.723540933228244e-05, 'epoch': 0.45}\n",
            "{'loss': 0.0, 'learning_rate': 4.690766700109659e-05, 'epoch': 0.48}\n",
            "{'loss': 0.0, 'learning_rate': 4.656284173018144e-05, 'epoch': 0.51}\n",
            "{'loss': 0.0, 'learning_rate': 4.620120240391065e-05, 'epoch': 0.53}\n",
            "{'loss': 0.0, 'learning_rate': 4.5823031017752485e-05, 'epoch': 0.56}\n",
            "{'loss': 0.0, 'learning_rate': 4.542862245837821e-05, 'epoch': 0.59}\n",
            "{'loss': 0.0, 'learning_rate': 4.5018284273718336e-05, 'epoch': 0.61}\n",
            "{'loss': 0.0, 'learning_rate': 4.4592336433146e-05, 'epoch': 0.64}\n",
            "{'loss': 0.0, 'learning_rate': 4.415111107797445e-05, 'epoch': 0.67}\n",
            "{'loss': 0.0, 'learning_rate': 4.36949522624633e-05, 'epoch': 0.69}\n",
            "{'loss': 0.0, 'learning_rate': 4.3224215685535294e-05, 'epoch': 0.72}\n",
            "{'loss': 0.0, 'learning_rate': 4.273926841341302e-05, 'epoch': 0.75}\n",
            "{'loss': 0.0, 'learning_rate': 4.224048859339175e-05, 'epoch': 0.77}\n",
            "{'loss': 0.0, 'learning_rate': 4.172826515897146e-05, 'epoch': 0.8}\n",
            "{'loss': 0.0, 'learning_rate': 4.1202997526578276e-05, 'epoch': 0.83}\n",
            "{'loss': 0.0, 'learning_rate': 4.066509528411152e-05, 'epoch': 0.85}\n",
            "{'loss': 0.0, 'learning_rate': 4.011497787155938e-05, 'epoch': 0.88}\n",
            "{'loss': 0.0, 'learning_rate': 3.955307425393224e-05, 'epoch': 0.91}\n",
            "{'loss': 0.0, 'learning_rate': 3.897982258676867e-05, 'epoch': 0.93}\n",
            "{'loss': 0.0, 'learning_rate': 3.8395669874474915e-05, 'epoch': 0.96}\n",
            "{'loss': 0.0, 'learning_rate': 3.780107162176429e-05, 'epoch': 0.99}\n",
            "{'loss': 0.0, 'learning_rate': 3.719649147846832e-05, 'epoch': 1.01}\n",
            "{'loss': 0.0, 'learning_rate': 3.6582400877996546e-05, 'epoch': 1.04}\n",
            "{'loss': 0.0, 'learning_rate': 3.5959278669726935e-05, 'epoch': 1.07}\n",
            "{'loss': 0.0, 'learning_rate': 3.532761074561355e-05, 'epoch': 1.09}\n",
            "{'loss': 0.0, 'learning_rate': 3.4687889661302576e-05, 'epoch': 1.12}\n",
            "{'loss': 0.0, 'learning_rate': 3.4040614252052305e-05, 'epoch': 1.15}\n",
            "{'loss': 0.0, 'learning_rate': 3.338628924375638e-05, 'epoch': 1.17}\n",
            "{'loss': 0.0, 'learning_rate': 3.272542485937369e-05, 'epoch': 1.2}\n",
            "{'loss': 0.0, 'learning_rate': 3.205853642107192e-05, 'epoch': 1.23}\n",
            "{'loss': 0.0, 'learning_rate': 3.138614394839476e-05, 'epoch': 1.25}\n",
            "{'loss': 0.0, 'learning_rate': 3.0708771752766394e-05, 'epoch': 1.28}\n",
            "{'loss': 0.0, 'learning_rate': 3.002694802864912e-05, 'epoch': 1.31}\n",
            "{'loss': 0.0, 'learning_rate': 2.9341204441673266e-05, 'epoch': 1.33}\n",
            "{'loss': 0.0, 'learning_rate': 2.8652075714060295e-05, 'epoch': 1.36}\n",
            "{'loss': 0.0, 'learning_rate': 2.7960099207662532e-05, 'epoch': 1.39}\n",
            "{'loss': 0.0, 'learning_rate': 2.726581450494451e-05, 'epoch': 1.41}\n",
            "{'loss': 0.0, 'learning_rate': 2.656976298823284e-05, 'epoch': 1.44}\n",
            "{'loss': 0.0, 'learning_rate': 2.587248741756253e-05, 'epoch': 1.47}\n",
            "{'loss': 0.0, 'learning_rate': 2.517453150744904e-05, 'epoch': 1.49}\n",
            "{'loss': 0.0, 'learning_rate': 2.447643950291608e-05, 'epoch': 1.52}\n",
            "{'loss': 0.0, 'learning_rate': 2.377875575510967e-05, 'epoch': 1.55}\n",
            "{'loss': 0.0, 'learning_rate': 2.3082024296829536e-05, 'epoch': 1.57}\n",
            "{'loss': 0.0, 'learning_rate': 2.238678841830867e-05, 'epoch': 1.6}\n",
            "{'loss': 0.0, 'learning_rate': 2.1693590243571938e-05, 'epoch': 1.63}\n",
            "{'loss': 0.0, 'learning_rate': 2.1002970307704132e-05, 'epoch': 1.65}\n",
            "{'loss': 0.0, 'learning_rate': 2.031546713535688e-05, 'epoch': 1.68}\n",
            "{'loss': 0.0, 'learning_rate': 1.963161682082342e-05, 'epoch': 1.71}\n",
            "{'loss': 0.0, 'learning_rate': 1.895195261000831e-05, 'epoch': 1.73}\n",
            "{'loss': 0.0, 'learning_rate': 1.827700448461836e-05, 'epoch': 1.76}\n",
            "{'loss': 0.0, 'learning_rate': 1.7607298748898842e-05, 'epoch': 1.79}\n",
            "{'loss': 0.0, 'learning_rate': 1.6943357619237226e-05, 'epoch': 1.81}\n",
            "{'loss': 0.0, 'learning_rate': 1.6285698816954624e-05, 'epoch': 1.84}\n",
            "{'loss': 0.0, 'learning_rate': 1.56348351646022e-05, 'epoch': 1.87}\n",
            "{'loss': 0.0, 'learning_rate': 1.4991274186077632e-05, 'epoch': 1.89}\n",
            "{'loss': 0.0, 'learning_rate': 1.4355517710873184e-05, 'epoch': 1.92}\n",
            "{'loss': 0.0, 'learning_rate': 1.3728061482764238e-05, 'epoch': 1.95}\n",
            "{'loss': 0.0, 'learning_rate': 1.3109394773243117e-05, 'epoch': 1.97}\n",
            "{'loss': 0.0, 'learning_rate': 1.2500000000000006e-05, 'epoch': 2.0}\n",
            "{'loss': 0.0, 'learning_rate': 1.1900352350748026e-05, 'epoch': 2.03}\n",
            "{'loss': 0.0, 'learning_rate': 1.1310919412686247e-05, 'epoch': 2.05}\n",
            "{'loss': 0.0, 'learning_rate': 1.0732160807889211e-05, 'epoch': 2.08}\n",
            "{'loss': 0.0, 'learning_rate': 1.0164527834907467e-05, 'epoch': 2.11}\n",
            "{'loss': 0.0, 'learning_rate': 9.608463116858542e-06, 'epoch': 2.13}\n",
            "{'loss': 0.0, 'learning_rate': 9.064400256282757e-06, 'epoch': 2.16}\n",
            "{'loss': 0.0, 'learning_rate': 8.532763497032987e-06, 'epoch': 2.19}\n",
            "{'loss': 0.0, 'learning_rate': 8.013967393462094e-06, 'epoch': 2.21}\n",
            "{'loss': 0.0, 'learning_rate': 7.508416487165862e-06, 'epoch': 2.24}\n",
            "{'loss': 0.0, 'learning_rate': 7.016504991533726e-06, 'epoch': 2.27}\n",
            "{'loss': 0.0, 'learning_rate': 6.538616484352902e-06, 'epoch': 2.29}\n",
            "{'loss': 0.0, 'learning_rate': 6.075123608706093e-06, 'epoch': 2.32}\n",
            "{'loss': 0.0, 'learning_rate': 5.626387782395512e-06, 'epoch': 2.35}\n",
            "{'loss': 0.0, 'learning_rate': 5.192758916120236e-06, 'epoch': 2.37}\n",
            "{'loss': 0.0, 'learning_rate': 4.7745751406263165e-06, 'epoch': 2.4}\n",
            "{'loss': 0.0, 'learning_rate': 4.372162543042624e-06, 'epoch': 2.43}\n",
            "{'loss': 0.0, 'learning_rate': 3.985834912607894e-06, 'epoch': 2.45}\n",
            "{'loss': 0.0, 'learning_rate': 3.6158934959873353e-06, 'epoch': 2.48}\n",
            "{'loss': 0.0, 'learning_rate': 3.262626762369525e-06, 'epoch': 2.51}\n",
            "{'loss': 0.0, 'learning_rate': 2.9263101785268254e-06, 'epoch': 2.53}\n",
            "{'loss': 0.0, 'learning_rate': 2.6072059940146775e-06, 'epoch': 2.56}\n",
            "{'loss': 0.0, 'learning_rate': 2.3055630366772856e-06, 'epoch': 2.59}\n",
            "{'loss': 0.0, 'learning_rate': 2.0216165186191407e-06, 'epoch': 2.61}\n",
            "{'loss': 0.0, 'learning_rate': 1.7555878527937164e-06, 'epoch': 2.64}\n",
            "{'loss': 0.0, 'learning_rate': 1.5076844803522922e-06, 'epoch': 2.67}\n",
            " 89% 1000/1125 [1:05:05<07:58,  3.82s/it]12/17/2023 08:55:19 - INFO - pet.core.trainer - Saving model checkpoint to ./output/label_xfg/checkpoint-1000\n",
            "{'loss': 0.0, 'learning_rate': 1.2780997088875869e-06, 'epoch': 2.69}\n",
            "{'loss': 0.0, 'learning_rate': 1.067012561698319e-06, 'epoch': 2.72}\n",
            "{'loss': 0.0, 'learning_rate': 8.745876381922147e-07, 'epoch': 2.75}\n",
            "{'loss': 0.0, 'learning_rate': 7.009749855363456e-07, 'epoch': 2.77}\n",
            "{'loss': 0.0, 'learning_rate': 5.463099816548579e-07, 'epoch': 2.8}\n",
            "{'loss': 0.0, 'learning_rate': 4.107132296653549e-07, 'epoch': 2.83}\n",
            "{'loss': 0.0, 'learning_rate': 2.942904638361804e-07, 'epoch': 2.85}\n",
            "{'loss': 0.0, 'learning_rate': 1.9713246713805588e-07, 'epoch': 2.88}\n",
            "{'loss': 0.0, 'learning_rate': 1.193150004542204e-07, 'epoch': 2.91}\n",
            "{'loss': 0.0, 'learning_rate': 6.089874350439506e-08, 'epoch': 2.93}\n",
            "{'loss': 0.0, 'learning_rate': 2.192924752854042e-08, 'epoch': 2.96}\n",
            "{'loss': 0.0, 'learning_rate': 2.4368997673940297e-09, 'epoch': 2.99}\n",
            "100% 1125/1125 [1:13:10<00:00,  3.79s/it][INFO|trainer.py:2053] 2023-12-17 09:03:23,990 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4390.2996, 'train_samples_per_second': 4.1, 'train_steps_per_second': 0.256, 'train_loss': 1.9540397135416667, 'epoch': 3.0}\n",
            "100% 1125/1125 [1:13:10<00:00,  3.90s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      1.954\n",
            "  train_runtime            = 1:13:10.29\n",
            "  train_samples_per_second =        4.1\n",
            "  train_steps_per_second   =      0.256\n",
            "12/17/2023 09:03:23 - INFO - pet.core.trainer - Saving model checkpoint to ./output/label_xfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrpGmEOkafz3",
        "outputId": "bbd6a126-039c-45b3-b527-838b1018deb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.0\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0) (3.1.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0) (0.42.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0) (3.27.9)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0)\n",
            "  Downloading lit-17.0.6.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.6-py3-none-any.whl size=93255 sha256=a394924841fa92acd5758c754c8749315d522343421aa4aee9c28db1a9b20c16\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/dd/04/47d42976a6a86dc2ab66d7518621ae96f43452c8841d74758a\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-17.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.0 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.30.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC7wqT2pZybi",
        "outputId": "583d7d20-1143-42a3-bd62-811ffa33fdae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.30.1\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.1) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.1) (2023.11.17)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/xfg-paper\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSxxqnPHY0wy",
        "outputId": "e32a4deb-5195-448b-cfa7-bb6373fbb5e4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/xfg-paper\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.27.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.35.2)\n",
            "Collecting datasets>=2.10.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.19.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.4.4 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.20.3)\n",
            "Collecting cpm-kernels (from -r requirements.txt (line 8))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.42.1)\n",
            "Collecting rouge-chinese (from -r requirements.txt (line 11))\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.8.1)\n",
            "Collecting gradio (from -r requirements.txt (line 13))\n",
            "  Downloading gradio-4.10.0-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdtex2html (from -r requirements.txt (line 14))\n",
            "  Downloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting uvicorn (from -r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from -r requirements.txt (line 16))\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 17))\n",
            "  Downloading sse_starlette-1.8.2-py3-none-any.whl (8.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Collecting multiprocess (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.9.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.4.4->-r requirements.txt (line 6))\n",
            "  Downloading tyro-0.6.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 12)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 12)) (1.3.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (4.2.2)\n",
            "Collecting ffmpy (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.3 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading gradio_client-0.7.3-py3-none-any.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (9.4.0)\n",
            "Collecting pydantic>=2.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (0.9.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.3->gradio->-r requirements.txt (line 13))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mdtex2html->-r requirements.txt (line 14)) (3.5.1)\n",
            "Collecting latex2mathml (from mdtex2html->-r requirements.txt (line 14))\n",
            "  Downloading latex2mathml-3.77.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->-r requirements.txt (line 16)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->-r requirements.txt (line 16))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 16)) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 16)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->-r requirements.txt (line 16)) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.10.0->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio->-r requirements.txt (line 13))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio->-r requirements.txt (line 13))\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13)) (13.7.0)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.4.4->-r requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.4.4->-r requirements.txt (line 6))\n",
            "  Downloading shtab-1.6.5-py3-none-any.whl (13 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio->-r requirements.txt (line 13))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r requirements.txt (line 13)) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=1c0abcc6fa1d8c32a4d324aeb7639756fb82c7cf38be925662bc96a5f0b168d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: sentencepiece, pydub, ffmpy, cpm-kernels, websockets, typing-extensions, tomlkit, shtab, shellingham, semantic-version, rouge-chinese, python-multipart, pyarrow-hotfix, orjson, latex2mathml, h11, docstring-parser, dill, colorama, annotated-types, aiofiles, uvicorn, starlette, pydantic-core, multiprocess, mdtex2html, httpcore, tyro, pydantic, httpx, accelerate, gradio-client, fastapi, datasets, trl, sse-starlette, peft, gradio\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.25.0 aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 cpm-kernels-1.0.11 datasets-2.15.0 dill-0.3.7 docstring-parser-0.15 fastapi-0.105.0 ffmpy-0.3.1 gradio-4.10.0 gradio-client-0.7.3 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 latex2mathml-3.77.0 mdtex2html-1.2.0 multiprocess-0.70.15 orjson-3.9.10 peft-0.7.1 pyarrow-hotfix-0.6 pydantic-2.5.2 pydantic-core-2.14.5 pydub-0.25.1 python-multipart-0.0.6 rouge-chinese-1.0.3 semantic-version-2.10.0 sentencepiece-0.1.99 shellingham-1.5.4 shtab-1.6.5 sse-starlette-1.8.2 starlette-0.27.0 tomlkit-0.12.0 trl-0.7.4 typing-extensions-4.9.0 tyro-0.6.0 uvicorn-0.24.0.post1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EQZxWZYVvE7",
        "outputId": "dcb22529-8271-4214-b632-a941262d0955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'xfg-paper'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 117 (delta 25), reused 100 (delta 17), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (117/117), 15.67 MiB | 23.53 MiB/s, done.\n",
            "Resolving deltas: 100% (25/25), done.\n",
            "Cloning into 'chatglm2-6b'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (70/70), done.\u001b[K\n",
            "remote: Total 186 (delta 53), reused 15 (delta 15), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (186/186), 1.95 MiB | 5.77 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Filtering content: 100% (8/8), 11.63 GiB | 133.81 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KMnO4-zx/xfg-paper.git\n",
        "!git clone https://huggingface.co/THUDM/chatglm2-6b"
      ]
    }
  ]
}